#!/usr/bin/python
'''
Utility to push this Arch hosts package and AUR caches to other host[s]
to avoid those other hosts having to download the same new package lists
and updated packages, at least for common packages. Requires root ssh
access to other hosts (it is easier with a auth key). Requires cower to
be installed on all target hosts.
'''
# Author: Mark Blakeney, Mar 2017.

import sys, os, platform, subprocess, argparse, tempfile, pickle
import ruamel.yaml as yaml
from pathlib import Path

PROG = Path(sys.argv[0]).stem

# Conf file containing gesture commands.
# Search first for user file then system file.
CONFNAME = f'{PROG}.conf'
USERCNF = os.getenv('XDG_CONFIG_HOME', os.path.expanduser('~/.config'))
USERCACHE = os.getenv('XDG_CACHE_HOME', os.path.expanduser('~/.cache'))
CONFDIRS = (USERCNF, '/etc')

# Process command line options
opt = argparse.ArgumentParser(description=__doc__)
opt.add_argument('-n', '--dryrun', action='store_true', help='dry run only')
opt.add_argument('-m', '--no-machcheck', action='store_true',
        help='do not check machine type compatibility')
opt.add_argument('-s', '--series', action='store_true',
        help='Run remote host updates in series not parallel')
opt.add_argument('-c', '--conffile',
        help='alternative configuration file')
opt.add_argument('hosts', nargs='+', help='hosts to update')
opt.add_argument('--env', help=argparse.SUPPRESS)
args = opt.parse_args()

# If not invoked as root then re-invoke ourself using sudo
if os.geteuid() != 0:
    # Search for configuration file. Use file given as command line
    # argument, else look for file in search dir order.
    if args.conffile:
        conffile = Path(args.conffile)
        if not conffile.exists():
            sys.exit(f'Conf file "{conffile}" does not exist.')
    else:
        for confdir in CONFDIRS:
            conffile = Path(confdir, CONFNAME)
            if conffile.exists():
                break
        else:
            dirs = ' or '.join(CONFDIRS)
            sys.exit(f'No file {CONFNAME} in {dirs}.')

    with conffile.open() as fp:
        conf = yaml.safe_load(fp)

    clonedir = Path(conf.get('clonedir', f'{USERCACHE}/pacaur')).expanduser()

    # Save ordinary user environment to reference for running as root
    fp = tempfile.NamedTemporaryFile()
    pickle.dump(clonedir, fp)
    fp.flush()

    # Pass ssh auth so that root uses sudo user's ssh cached key and
    # also pass user environment
    sock = os.getenv('SSH_AUTH_SOCK')
    cmd = ['/usr/bin/sudo', f'SSH_AUTH_SOCK={sock}'] + sys.argv + \
            [f'--env={fp.name}']
    sys.exit(subprocess.run(cmd).returncode)

SUDO_USER = os.getenv('SUDO_USER')
if not SUDO_USER:
    sys.exit('Do not run as root. Run directly as your normal user.')

# Load calling user's environment for reference. This is a hidden
# argument passed programmatically from above.
with open(args.env, 'rb') as fp:
    clonedir = pickle.load(fp)

# Define paths of interest for pacman
PACLIST = Path('/var/lib/pacman/sync')
PACPKGS = Path('/var/cache/pacman/pkg')

HOST = platform.node()
MACH = platform.machine()
dryrun = '-n ' if args.dryrun else ''

def synchost(host):
    'Process given host'
    if not args.no_machcheck:
        res = subprocess.run(f'/usr/bin/ssh {host} uname -m'.split(),
                universal_newlines=True, stdout=subprocess.PIPE)

        if res.returncode != 0:
            print(f'{HOST} failed to ssh to {host}.', file=sys.stderr)
            print(f'Have you set up root ssh access to {host}?',
                    file=sys.stderr)
            return

        hostmach = res.stdout.strip()
        if hostmach != MACH:
            print(f'This {HOST} type={MACH} does not match '
                    f'{host} type={hostmach}.', file=sys.stderr)
            return

    # Push the current package lists to the host then work out what
    # package updates are required by this host. Then push all new
    # packages it requires that we already hold, including AUR files.
    print(f'{HOST} syncing {MACH} package lists to {host} ..')
    res = subprocess.run(
            f'/usr/bin/rsync -aRO --info=name1 {dryrun}'
            f'{PACLIST} {host}:/'.split())

    if res.returncode != 0:
        print(f'{HOST} failed to sync package lists to {host}.',
                file=sys.stderr)
        return

    print(f'Getting list of required package updates from {host} ..')
    res = subprocess.run(f'/usr/bin/ssh {host} pacman -Qu; '
            f'cower -u --color=never'.split(),
            universal_newlines=True, stdout=subprocess.PIPE)

    filelist = []
    pkg = None
    for line in res.stdout.strip().splitlines():
        if line.startswith(':'):
            aur = True
            junk, name, oldver, junk, newver = line.split()
        else:
            aur = False
            name, oldver, junk, newver = line.split()

        pkg = f'{name}-{newver}'

        # f'{host} requires update of {name} from {oldver} to {newver}'
        if aur:
            # Sync the entire clone dir for an AUR package
            tpath = clonedir.joinpath(name)
            desc = f'AUR {name}/'
        else:
            # Can't be sure of the file (package) extension so get the
            # latest file by time.
            tpath = max(PACPKGS.glob(f'{pkg}-*'),
                    key=lambda p: p.stat().st_mtime, default=None)
            desc = pkg

        if tpath and tpath.exists():
            print(f'> {HOST} has {desc} for {host}')
            filelist.append(tpath)
        else:
            print(f'> {HOST} does not have {desc} for {host}')

    if filelist:
        print(f'{HOST} syncing updated packages to {host} ..')
        with tempfile.NamedTemporaryFile() as fp:
            fp.writelines(bytes(l) + b'\n' for l in filelist)
            fp.flush()
            subprocess.run(
                f'/usr/bin/rsync -arRO --info=name1 {dryrun}'
                f'--files-from {fp.name} / {host}:/'.split())
    elif pkg:
        print(f'{HOST} does not have any packages {host} requires.')
    else:
        print(f'{host} is already up to date.')

# May as well do in same process if only a single remote host
do_in_series = args.series or len(args.hosts) == 1

# For each host specified on the command line ..
for host in args.hosts:
    if do_in_series:
        synchost(host)
    else:
        # Do in parallel ..
        from multiprocessing import Process
        p = Process(target=synchost, args=(host,))
        p.start()
